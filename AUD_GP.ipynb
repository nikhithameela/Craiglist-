{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e8a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 = str1+\" \"+ele  \n",
    "    \n",
    "    # return string  \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d82cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Nikhi\\OneDrive - purdue.edu\\Mod 2\\Analyzing Unstructured Data\\Group Project\\UpdatedResumeDataSet.csv\\UpdatedResumeDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64eb1175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(962, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a7ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "df['target'] = labelencoder.fit_transform(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13ce0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts().sort_index()\n",
    "y=df[['target']]\n",
    "X=df[['Resume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83a5c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b53fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 1)\n",
      "(673, 1)\n",
      "(289, 1)\n",
      "(289, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28cff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data into training data \n",
    "#Tokenize the collection \n",
    "token_list=[]\n",
    "for i in X_train['Resume']:\n",
    "    token_list.append(nltk.word_tokenize(i))\n",
    "#print(len(token_list))\n",
    "#Lammetize the words \n",
    "lammetize_list=[]\n",
    "for i in token_list:\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_token=[lemmatizer.lemmatize(token) for token in i]\n",
    "    lammetize_list.append(lemmatized_token)\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_list=[]\n",
    "for i in lammetize_list:\n",
    "    stop_words_removed = [token for token in i if not token in stopwords.words('english') if token.isalpha()]\n",
    "    stop_list.append(stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82096ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4276\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operating</th>\n",
       "      <th>system</th>\n",
       "      <th>ubuntu</th>\n",
       "      <th>windows</th>\n",
       "      <th>other</th>\n",
       "      <th>tool</th>\n",
       "      <th>tableau</th>\n",
       "      <th>svn</th>\n",
       "      <th>beyond</th>\n",
       "      <th>details</th>\n",
       "      <th>...</th>\n",
       "      <th>formerly</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>prompt</th>\n",
       "      <th>labour</th>\n",
       "      <th>investigation</th>\n",
       "      <th>scale</th>\n",
       "      <th>requested</th>\n",
       "      <th>temporary</th>\n",
       "      <th>wise</th>\n",
       "      <th>mainly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   operating  system  ubuntu  windows  other  tool  tableau       svn  beyond  \\\n",
       "0        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.000000     0.0   \n",
       "1        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.000000     0.0   \n",
       "2        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.025934     0.0   \n",
       "3        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.000000     0.0   \n",
       "4        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.000000     0.0   \n",
       "\n",
       "   details  ...  formerly  preprocessing  prompt  labour  investigation  \\\n",
       "0      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "1      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "2      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "3      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "4      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "\n",
       "   scale  requested  temporary  wise  mainly  \n",
       "0    0.0        0.0        0.0   0.0     0.0  \n",
       "1    0.0        0.0        0.0   0.0     0.0  \n",
       "2    0.0        0.0        0.0   0.0     0.0  \n",
       "3    0.0        0.0        0.0   0.0     0.0  \n",
       "4    0.0        0.0        0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 4276 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list=[]\n",
    "for i in stop_list:\n",
    "    sentence_list=listToString(i)\n",
    "    final_list.append(sentence_list)\n",
    "#TFIDF min_df=3 and include 2-gram \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(min_df=5)\n",
    "v1=vectorizer.fit(final_list)\n",
    "v2=vectorizer.transform(final_list)\n",
    "print(len(v1.vocabulary_.keys()))\n",
    "X_train=pd.DataFrame(v2.toarray(),columns=v1.vocabulary_.keys())\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182dd0d",
   "metadata": {},
   "source": [
    "## Preprocessing Done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f5e4d",
   "metadata": {},
   "source": [
    "### Preparing the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a3f932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data into training data \n",
    "#Tokenize the collection \n",
    "token_list_test=[]\n",
    "for i in X_test['Resume']:\n",
    "    token_list_test.append(nltk.word_tokenize(i))\n",
    "#print(len(token_list))\n",
    "#Lammetize the words \n",
    "lammetize_list_test=[]\n",
    "for i in token_list_test:\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_token=[lemmatizer.lemmatize(token) for token in i]\n",
    "    lammetize_list_test.append(lemmatized_token)\n",
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_list_test=[]\n",
    "for i in lammetize_list_test:\n",
    "    stop_words_removed = [token for token in i if not token in stopwords.words('english') if token.isalpha()]\n",
    "    stop_list_test.append(stop_words_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dfcc225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operating</th>\n",
       "      <th>system</th>\n",
       "      <th>ubuntu</th>\n",
       "      <th>windows</th>\n",
       "      <th>other</th>\n",
       "      <th>tool</th>\n",
       "      <th>tableau</th>\n",
       "      <th>svn</th>\n",
       "      <th>beyond</th>\n",
       "      <th>details</th>\n",
       "      <th>...</th>\n",
       "      <th>formerly</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>prompt</th>\n",
       "      <th>labour</th>\n",
       "      <th>investigation</th>\n",
       "      <th>scale</th>\n",
       "      <th>requested</th>\n",
       "      <th>temporary</th>\n",
       "      <th>wise</th>\n",
       "      <th>mainly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   operating  system  ubuntu  windows  other  tool  tableau  svn  beyond  \\\n",
       "0        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.0     0.0   \n",
       "1        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.0     0.0   \n",
       "2        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.0     0.0   \n",
       "3        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.0     0.0   \n",
       "4        0.0     0.0     0.0      0.0    0.0   0.0      0.0  0.0     0.0   \n",
       "\n",
       "   details  ...  formerly  preprocessing  prompt  labour  investigation  \\\n",
       "0      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "1      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "2      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "3      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "4      0.0  ...       0.0            0.0     0.0     0.0            0.0   \n",
       "\n",
       "   scale  requested  temporary  wise  mainly  \n",
       "0    0.0        0.0   0.066912   0.0     0.0  \n",
       "1    0.0        0.0   0.000000   0.0     0.0  \n",
       "2    0.0        0.0   0.000000   0.0     0.0  \n",
       "3    0.0        0.0   0.000000   0.0     0.0  \n",
       "4    0.0        0.0   0.000000   0.0     0.0  \n",
       "\n",
       "[5 rows x 4276 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_test=[]\n",
    "for i in stop_list_test:\n",
    "    sentence_list=listToString(i)\n",
    "    final_list_test.append(sentence_list)\n",
    "#Changing it w.r.t Tfidf vector \n",
    "v_test=vectorizer.transform(final_list_test)\n",
    "X_test=pd.DataFrame(v_test.toarray(),columns=v1.vocabulary_.keys())\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5876e",
   "metadata": {},
   "source": [
    "### Model Development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd853db3",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7512c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model Accuracy:: 91.6955%\n",
      "[0.00319434 0.00504505 0.00636596 0.79063666 0.00699991 0.00411881\n",
      " 0.01122761 0.00613479 0.01526078 0.00679081 0.00828858 0.00415108\n",
      " 0.00977556 0.00856221 0.00353724 0.03502325 0.00651351 0.00526119\n",
      " 0.00555432 0.00508201 0.01012819 0.00439054 0.00725091 0.01698834\n",
      " 0.01371836]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "## Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBmodel = MultinomialNB()\n",
    "# training\n",
    "NBmodel.fit(X_train, Y_train)\n",
    "y_pred_NB = NBmodel.predict(X_test)\n",
    "# evaluation\n",
    "acc_NB = accuracy_score(Y_test, y_pred_NB)\n",
    "print(\"Naive Bayes model Accuracy:: {:.4f}%\".format(acc_NB*100))\n",
    "y_pred_prob_nb = NBmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_nb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18380fe8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f31af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit model Accuracy:: 98.9619%\n",
      "[0.01381395 0.01717804 0.01570527 0.59715075 0.01718739 0.01184264\n",
      " 0.01884176 0.01509224 0.01927823 0.0167682  0.01485967 0.01305056\n",
      " 0.02234466 0.01397058 0.01241156 0.02650485 0.01726322 0.01363095\n",
      " 0.01519109 0.01179239 0.01721641 0.01297328 0.01741791 0.02597929\n",
      " 0.0225351 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logitmodel = LogisticRegression()\n",
    "# training\n",
    "Logitmodel.fit(X_train, Y_train)\n",
    "y_pred_logit = Logitmodel.predict(X_test)\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_logit = accuracy_score(Y_test, y_pred_logit)\n",
    "print(\"Logit model Accuracy:: {:.4f}%\".format(acc_logit*100))\n",
    "y_pred_prob_logit = Logitmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_logit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c07f2",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42d09dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Accuracy: 90.3114%\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "DTmodel = DecisionTreeClassifier(min_samples_leaf=15,random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "DTmodel.fit(X_train, Y_train)\n",
    "y_pred_DT = DTmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_DT = accuracy_score(Y_test, y_pred_DT)\n",
    "print(\"Decision Tree Model Accuracy: {:.4f}%\".format(acc_DT*100))\n",
    "y_pred_prob_dt = DTmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_dt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36324965",
   "metadata": {},
   "source": [
    "### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5a6fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-4ce8e5bd9cf5>:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RFmodel.fit(X_train, Y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Accuracy: 98.9619%\n",
      "[[0.01257841 0.02184149 0.00923925 ... 0.00881198 0.02434495 0.01041033]\n",
      " [0.01733796 0.02681144 0.01432071 ... 0.01821479 0.03856568 0.02071755]\n",
      " [0.02148637 0.03617315 0.01435552 ... 0.02095914 0.57284034 0.01818436]\n",
      " ...\n",
      " [0.02023155 0.034377   0.01476275 ... 0.0216357  0.05293243 0.0218633 ]\n",
      " [0.02678623 0.0455098  0.01629178 ... 0.02891813 0.04583765 0.03650271]\n",
      " [0.00635249 0.01251627 0.00568129 ... 0.01020861 0.01562373 0.00747624]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "RFmodel = RandomForestClassifier(n_estimators=100, max_depth=15, bootstrap=True, random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "RFmodel.fit(X_train, Y_train)\n",
    "y_pred_RF = RFmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_RF = accuracy_score(Y_test, y_pred_RF)\n",
    "print(\"Random Forest Model Accuracy: {:.4f}%\".format(acc_RF*100))\n",
    "y_pred_prob_rf = RFmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b217ff",
   "metadata": {},
   "source": [
    "### Xgboost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e9504c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Random Forest Model Accuracy: 99.3080%\n",
      "[[5.3137308e-04 4.4558811e-04 6.8680302e-04 ... 4.6680961e-04\n",
      "  1.1671365e-03 6.0045754e-04]\n",
      " [8.4786845e-04 1.4594939e-03 9.9065143e-04 ... 7.4070628e-04\n",
      "  7.3935761e-04 4.8146606e-03]\n",
      " [9.8041455e-05 3.9738195e-04 1.0416552e-04 ... 8.4701314e-05\n",
      "  9.9161303e-01 8.4463434e-05]\n",
      " ...\n",
      " [5.1715469e-04 1.0332038e-03 5.4945797e-04 ... 4.9518252e-04\n",
      "  2.8547538e-03 5.7256583e-04]\n",
      " [1.3150903e-03 1.0785164e-03 9.8097685e-04 ... 7.2777580e-04\n",
      "  4.3217928e-04 8.7529246e-04]\n",
      " [2.5767225e-04 2.2758610e-04 5.2053439e-03 ... 2.0310495e-04\n",
      "  9.8036358e-04 2.8061567e-04]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgmodel = XGBClassifier(random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "xgmodel.fit(X_train, Y_train)\n",
    "y_pred_xg = xgmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_xg = accuracy_score(Y_test, y_pred_xg)\n",
    "print(\"Xgboost Classifier Model Accuracy: {:.4f}%\".format(acc_xg*100))\n",
    "y_pred_prob_xg = xgmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_xg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b29ab",
   "metadata": {},
   "source": [
    "### Light GBM Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75f72c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GMB Classifier Model Accuracy: 99.6540%\n",
      "[2.47310472e-07 1.03691051e-06 3.90863311e-07 9.99984195e-01\n",
      " 5.05927342e-07 5.79996676e-07 3.52000609e-07 7.59502136e-07\n",
      " 6.53876582e-07 5.75071039e-07 9.56565532e-07 4.22409233e-07\n",
      " 8.22910914e-07 8.50592353e-07 8.50103112e-07 9.06481693e-07\n",
      " 8.19447031e-07 5.59629307e-07 5.94332802e-07 5.84609478e-07\n",
      " 6.10846725e-07 4.89885364e-07 3.46920910e-07 1.08524043e-06\n",
      " 8.03485990e-07]\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgbmodel = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "lgbmodel.fit(X_train, Y_train, verbose=20,eval_metric='logloss')\n",
    "y_pred_lgb = lgbmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_lgb = accuracy_score(Y_test, y_pred_lgb)\n",
    "print(\"Light GMB Classifier Model Accuracy: {:.4f}%\".format(acc_lgb*100))\n",
    "y_pred_prob_lgb = lgbmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_lgb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d84baa",
   "metadata": {},
   "source": [
    "### CAT Boost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98a06640",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.077545\n",
      "0:\tlearn: 3.0990674\ttotal: 320ms\tremaining: 5m 19s\n",
      "20:\tlearn: 1.9548077\ttotal: 4.35s\tremaining: 3m 22s\n",
      "40:\tlearn: 1.2744040\ttotal: 8.35s\tremaining: 3m 15s\n",
      "60:\tlearn: 0.8894125\ttotal: 12.6s\tremaining: 3m 13s\n",
      "80:\tlearn: 0.6315302\ttotal: 17.3s\tremaining: 3m 15s\n",
      "100:\tlearn: 0.4498025\ttotal: 22.1s\tremaining: 3m 17s\n",
      "120:\tlearn: 0.3225946\ttotal: 26.7s\tremaining: 3m 14s\n",
      "140:\tlearn: 0.2464941\ttotal: 31.4s\tremaining: 3m 11s\n",
      "160:\tlearn: 0.1951020\ttotal: 35.9s\tremaining: 3m 7s\n",
      "180:\tlearn: 0.1546790\ttotal: 40.5s\tremaining: 3m 3s\n",
      "200:\tlearn: 0.1277455\ttotal: 45.6s\tremaining: 3m 1s\n",
      "220:\tlearn: 0.1087157\ttotal: 50.8s\tremaining: 2m 58s\n",
      "240:\tlearn: 0.0912137\ttotal: 55.9s\tremaining: 2m 55s\n",
      "260:\tlearn: 0.0806826\ttotal: 1m\tremaining: 2m 51s\n",
      "280:\tlearn: 0.0705201\ttotal: 1m 5s\tremaining: 2m 48s\n",
      "300:\tlearn: 0.0627255\ttotal: 1m 10s\tremaining: 2m 44s\n",
      "320:\tlearn: 0.0550937\ttotal: 1m 15s\tremaining: 2m 40s\n",
      "340:\tlearn: 0.0492076\ttotal: 1m 22s\tremaining: 2m 39s\n",
      "360:\tlearn: 0.0445799\ttotal: 1m 28s\tremaining: 2m 37s\n",
      "380:\tlearn: 0.0405389\ttotal: 1m 35s\tremaining: 2m 35s\n",
      "400:\tlearn: 0.0371091\ttotal: 1m 41s\tremaining: 2m 32s\n",
      "420:\tlearn: 0.0340683\ttotal: 1m 47s\tremaining: 2m 28s\n",
      "440:\tlearn: 0.0315898\ttotal: 1m 53s\tremaining: 2m 24s\n",
      "460:\tlearn: 0.0293759\ttotal: 1m 59s\tremaining: 2m 19s\n",
      "480:\tlearn: 0.0277736\ttotal: 2m 4s\tremaining: 2m 14s\n",
      "500:\tlearn: 0.0261081\ttotal: 2m 10s\tremaining: 2m 9s\n",
      "520:\tlearn: 0.0246137\ttotal: 2m 15s\tremaining: 2m 4s\n",
      "540:\tlearn: 0.0232872\ttotal: 2m 21s\tremaining: 1m 59s\n",
      "560:\tlearn: 0.0217996\ttotal: 2m 27s\tremaining: 1m 55s\n",
      "580:\tlearn: 0.0206365\ttotal: 2m 34s\tremaining: 1m 51s\n",
      "600:\tlearn: 0.0196394\ttotal: 2m 40s\tremaining: 1m 46s\n",
      "620:\tlearn: 0.0187046\ttotal: 2m 46s\tremaining: 1m 41s\n",
      "640:\tlearn: 0.0178449\ttotal: 2m 51s\tremaining: 1m 36s\n",
      "660:\tlearn: 0.0171074\ttotal: 2m 57s\tremaining: 1m 31s\n",
      "680:\tlearn: 0.0163863\ttotal: 3m 3s\tremaining: 1m 26s\n",
      "700:\tlearn: 0.0158347\ttotal: 3m 9s\tremaining: 1m 20s\n",
      "720:\tlearn: 0.0152177\ttotal: 3m 16s\tremaining: 1m 15s\n",
      "740:\tlearn: 0.0146136\ttotal: 3m 23s\tremaining: 1m 10s\n",
      "760:\tlearn: 0.0141232\ttotal: 3m 30s\tremaining: 1m 6s\n",
      "780:\tlearn: 0.0136000\ttotal: 3m 36s\tremaining: 1m\n",
      "800:\tlearn: 0.0131194\ttotal: 3m 42s\tremaining: 55.3s\n",
      "820:\tlearn: 0.0127272\ttotal: 3m 48s\tremaining: 49.8s\n",
      "840:\tlearn: 0.0123478\ttotal: 3m 54s\tremaining: 44.4s\n",
      "860:\tlearn: 0.0120003\ttotal: 4m\tremaining: 38.8s\n",
      "880:\tlearn: 0.0116514\ttotal: 4m 6s\tremaining: 33.3s\n",
      "900:\tlearn: 0.0113203\ttotal: 4m 13s\tremaining: 27.8s\n",
      "920:\tlearn: 0.0109930\ttotal: 4m 21s\tremaining: 22.4s\n",
      "940:\tlearn: 0.0106810\ttotal: 4m 29s\tremaining: 16.9s\n",
      "960:\tlearn: 0.0103809\ttotal: 4m 36s\tremaining: 11.2s\n",
      "980:\tlearn: 0.0100321\ttotal: 4m 43s\tremaining: 5.48s\n",
      "999:\tlearn: 0.0097651\ttotal: 4m 48s\tremaining: 0us\n",
      "Light GMB Classifier Model Accuracy: 99.6540%\n",
      "[1.34909948e-04 2.26423562e-04 1.77435159e-04 9.94184676e-01\n",
      " 2.23481292e-04 1.54262568e-04 2.97320246e-04 2.06614195e-04\n",
      " 3.11898015e-04 1.26486825e-04 1.84683403e-04 1.68754424e-04\n",
      " 1.69389373e-04 1.94375779e-04 1.52987681e-04 2.33595996e-04\n",
      " 3.19118457e-04 3.92831936e-04 3.70886727e-04 2.55180009e-04\n",
      " 1.71273617e-04 1.67672075e-04 2.73252766e-04 5.94820687e-04\n",
      " 3.07669361e-04]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "catmodel = CatBoostClassifier(random_state=0) ## number of trees and number of layers/depth\n",
    "#training\n",
    "catmodel.fit(X_train, Y_train, verbose=20)\n",
    "y_pred_cat = catmodel.predict(X_test)\n",
    "#evaluation\n",
    "acc_cat = accuracy_score(Y_test, y_pred_cat)\n",
    "print(\"Light GMB Classifier Model Accuracy: {:.4f}%\".format(acc_cat*100))\n",
    "y_pred_prob_cat = catmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_cat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f70e8f",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8af55bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model Accuracy: 98.9619%\n",
      "[0.00000000e+000 4.55114054e-226 7.42228163e-197 9.99999992e-001\n",
      " 9.64957015e-284 2.27765274e-294 8.10643428e-172 2.42602206e-195\n",
      " 0.00000000e+000 0.00000000e+000 3.78941871e-219 3.76466386e-264\n",
      " 0.00000000e+000 8.19904303e-009 0.00000000e+000 0.00000000e+000\n",
      " 1.66151441e-106 1.52499044e-252 6.73010271e-197 2.17080459e-086\n",
      " 0.00000000e+000 2.67131413e-319 5.26175983e-115 0.00000000e+000\n",
      " 2.35295953e-114]\n"
     ]
    }
   ],
   "source": [
    "## Neural Network and Deep Learning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "DLmodel = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(20,6), random_state=1)\n",
    "# training\n",
    "DLmodel.fit(X_train, Y_train)\n",
    "y_pred_DL= DLmodel.predict(X_test)\n",
    "# evaluation\n",
    "acc_DL = accuracy_score(Y_test, y_pred_DL)\n",
    "print(\"DL model Accuracy: {:.4f}%\".format(acc_DL*100))\n",
    "y_pred_prob_dl = DLmodel.predict_proba(X_test)\n",
    "print(y_pred_prob_dl[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028e112",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cfcde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data as this needs different inputs \n",
    "y_lstm=df[['target']]\n",
    "X_lstm=df[['Resume']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_lstm,X_test_lstm,Y_train_lstm,Y_test_lstm=train_test_split(X_lstm,y_lstm,test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa452c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding each label and transforming both train and test datasets \n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "tokenized_list= [nltk.word_tokenize(doc.lower()) for doc in df['Resume']]\n",
    "tokenized_list_test= [nltk.word_tokenize(doc.lower()) for doc in X_test_lstm['Resume']]\n",
    "tokenized_list_train= [nltk.word_tokenize(doc.lower()) for doc in X_train_lstm['Resume']]\n",
    "# A set for all possible words\n",
    "words = [j for i in tokenized_list for j in i]\n",
    "total_words=len(words)\n",
    "index_encoder = LabelEncoder()\n",
    "index_encoder = index_encoder.fit(words) # define vocabulary\n",
    "X_train_lstm = [index_encoder.transform(doc) for doc in tokenized_list_train]\n",
    "X_test_lstm = [index_encoder.transform(doc) for doc in tokenized_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "from keras.preprocessing import sequence \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers import LSTM\n",
    "max_features = total_words\n",
    "maxlen = 2000\n",
    "batch_size = 50\n",
    "# padding\n",
    "x_train_lstm = sequence.pad_sequences(X_train_lstm, maxlen=maxlen)\n",
    "x_test_lstm = sequence.pad_sequences(X_test_lstm, maxlen=maxlen)\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 40, input_length=maxlen))\n",
    "model.add(LSTM(1000, dropout=0.20, recurrent_dropout=0.20))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_lstm, Y_train_lstm, batch_size=batch_size, epochs=25, validation_data=(x_test_lstm, Y_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b61f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
